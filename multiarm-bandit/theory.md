Im Kern geht es bei den sogenannten Multi-Armed Bandits um Entscheidungen unter Unsicherheit.
Man stelle sich vor, man steht in einer Spielhalle mit mehreren Spielautomaten – jeder davon hat eine unbekannte, aber feste Gewinnwahrscheinlichkeit.
Das Ziel ist klar: Mit möglichst wenigen Versuchen herausfinden, welcher Automat (oder in unserem Fall welches Musikgenre) am häufigsten „gewinnt“.

Das Spannende daran: Zwischen Ausprobieren (Exploration) und Ausnutzen (Exploitation) muss stets abgewogen werden.
Soll man weiter experimentieren, um vielleicht ein besseres Genre zu entdecken?
Oder lieber bei dem bleiben, das bisher am häufigsten gefällt?

Genau diese Balance steht im Mittelpunkt vieler moderner Systeme – von Spotify und Netflix bis hin zu Online-Werbung, Medizin und Robotik.
Denn überall dort, wo Daten live entstehen und Entscheidungen dynamisch angepasst werden müssen, ist der Bandit-Ansatz eine hocheffiziente Lösung.

Unsere Simulationen machen dieses Prinzip greifbar:
Du übernimmst selbst die Rolle des Algorithmus, triffst Entscheidungen, sammelst Daten und lernst, welche Strategie dir den größten Erfolg bringt.
